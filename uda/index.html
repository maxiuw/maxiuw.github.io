<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UADA3D">
  <meta name="keywords" content="UADA3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.kth.se/profile/maciejw">Maciej Wozniak</a><sup>1,2</sup>, </span>
            <span class="author-block">
              <a href="">Mattias Hansson</a><sup>1</sup>, </span>
            <span class="author-block">
              <a href="">Marko Thiel</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.kth.se/profile/patric/">Patric Jensfelt</a><sup>1,2</sup>,
            </span>
            <!-- <span class="author-block">
              <a href="https://homes.cs.washington.edu/~curless/">Brian Curless</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jannekontkanen/">Janne Kontkanen</a><sup>1</sup>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KTH Royal Institute of Technology &nbsp; </span>
            <span class="author-block"><sup>2</sup>TU Hamburg &nbsp; </span>
            <!-- <span class="author-block"><sup>3</sup>University of Washington</span> -->

          </div>

          <h1 style="font-size:24px;font-weight:bold">Under review</h1>
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="static/pdfs/3d_moments.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2205.06255"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="#overview_video"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Supp Link. -->
              <span class="link-block">
                <a href="static/pdfs/supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supp</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/maxiuw/UADA3D"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline controls width="100%">
        <source src="static/videos/teaser2.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">3D Moments</span> turns a duplicate photo pair into space-time videos
      </h2>
    </div>
  </div>
</section> -->


<!-- make this section wider -->

<!-- <section class="hero is-light is-small">
  <div class="hero-body" >
    <div class="container" >
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-fullbody">
          <img src="figures/DTS1_page-0001.jpg" alt="overview_image">
        </div>

        <div class="item item-fullbody">
          <img src="figures/DTS1_page-0001.jpg" alt="overview_image">
        </div>

        <div class="item item-fullbody">
          <img src="figures/DTS1_page-0001.jpg" alt="overview_image">
        </div>

        <div class="item item-shiba">
          <image poster="" id="shiba" autoplay playsinline controls muted loop height="100%">
            <source src="figures/DTS1_page-0001.jpg" type="jpg">
          </image>
        </div>
      </div>
    </div>
  </div>
</section>
 -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this study, we address a gap in existing unsupervised domain adaptation approaches on LiDAR-based 3D object detection, which have predominantly concentrated on adapting between established, high-density autonomous driving datasets. We focus on sparser point clouds, capturing scenarios from different perspectives: not just from vehicles on the road but also from mobile robots on sidewalks, which encounter significantly different environmental conditions and sensor configurations. We introduce Unsupervised Adversarial Domain Adaptation for 3D Object Detection (\textbf{UADA3D}). UADA3D does not depend on pre-trained source models or teacher-student architectures. Instead, it uses an adversarial approach to directly learn domain-invariant features. We demonstrate its efficacy in various adaptation scenarios, showing significant improvements in both self-driving car and mobile robot domains.          </p>
        </div>
      </div>
    </div>
  </div>

</section>


    
    <!--/ Abstract. -->

<!-- <div align="center" style="margin-top:80px;" style="margin-bottom:120px;">
<img style='height: auto; width: 75%; object-fit: contain' src="static/images/overview.png" alt="overview_image">
</div>  -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <a id="overview_video"></a>
          <video id="teaser" playsinline controls width="100%">
            <source src="static/videos/overview_video.mp4" type="video/mp4">
          </video>
      </div>
    </div>
  </div>-->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
  <div class="item item-fullbody">
    <img src="figures/main.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <!-- <h3 class="title is-3"> MS3D++</h3> -->
  </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Detailed Implementation</h2>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <p>
            Detailed implementation of UADA3D
            The conditional module has the task of reducing the discrepancy between the condi-
            tional label distribution P (Ys|Xs) of the source and P (Yt|Xt) of the target. The label
            space Yi consists of class labels y ∈ RN ×K and 3D bounding boxes bi ∈ R7. The
            feature space X consists of point features F ∈ RN ×C (<a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a>) or the 2D BEV pseudo-
            image I ∈ Rw×h×C (Centerpoint). The domain discriminator gθD in Centerpoint has
            2D convolutional layers of 264, 256, 128, 1 while <a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a> uses an MLP with dimensions
            519, 512, 256, 128, 1. LeakyReLU is used for the activation functions with a sigmoid layer
            for the final domain prediction. A kernel size of 3 was chosen for Centerpoint, based
            on experiments shown in B.4. Note, that we do class-wise domain prediction, thus we
            have K discriminators corresponding to the number of classes (in our case K = 3, but
            it can be easily modified).          </p>
            <p>
              Detailed implementation of UADA3D<sub>Lm</sub>
              The primary role UADA3D<sub>Lm</sub> with the marginal feature discriminator is to minimize the discrepancy between the marginal 
              feature distributions of the source, denoted as P(X<sub>s</sub>), and the target, represented by P(X<sub>t</sub>). Here, X<sub>s</sub> and X<sub>t</sub> 
              symbolize the features extracted by the detection backbone from the two distinct domains. This approach ensures the extraction of domain invariant features. 
              The loss function of UADA3D<sub>Lm</sub> marginal alignment module is defined through Binary Cross Entropy.

              The output of the point-based detection backbone in <a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a> is given by $N$ point features with feature dimension $C$ and corresponding encodings.
               Point-wise discriminators can be utilized to identify the distribution these points are drawn from. The input to the proposed marginal discriminator $g_{\theta_D}$ is 
               given by point-wise center features obtained through set abstraction and downsampling layers. The discriminator is made up of $5$ fully connected layers 
               ($512,256,128,64,32,1$) that reduce the feature dimension from $C$ to $1$. LeakyReLU is used in the activation layers and a final sigmoid layer is used for domain prediction. 

              The backbone in Centerpoint~\cite{yin2021center} uses sparse convolutions to extract voxel-based features that are flattened into 2D BEV-features. 
              Therefore, the input to the view-based marginal discriminator is given by a pseudo image of feature dimension $C$ with spatial dimensions $w$ and $h$ that define 
              the 2D BEV-grid. Since 2D convolutions are more computationally demanding over the MLP on the heavily downsampled point cloud in <a href="https://github.com/yifanzhang713/<a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a>"><a href="https://github.com/yifanzhang713/IA-SSD">IA-SSD</a></a>D<sub>Lm</sub>, the 2D marginal discriminator 
              uses a $3$-layered CNN that reduces the feature dimension from $C$ to $1$ (256,256,128,1), using a kernel size of $3$ and a stride of $1$. Same as in the point-wise case, 
              the loss function of UADA3D<sub>Lm</sub> is defined through  Cross Entropy.        </p>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Gradient Reversal Layer</h2>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <p>
            
In ablation studies we tested two different strategies for \textit{GRL}-coefficient $\lambda$ on UADA3D and UADA3D\textsubscript{$\mathcal{L}m$}. 
Firstly, a stationary $\lambda=0.1$ was tested following the setting used for most adversarial UDA strategies in 2D object detection~\cite{chen2018domain, saito2019strong}.
 Secondly, we follow other approaches~\cite{ganin2015unsupervised,li2023domain} where $\lambda$ was increased over the training according to:

\begin{equation} \label{eq:params-grl}
    \lambda = \alpha (\frac{2}{1 + exp(-\gamma p)} - 1)
\end{equation}

where $\alpha \in [0,1]$ is a scaling factor that determines the final $\lambda$, $\gamma=10$ and $p$ is the training progress from start $0$ to finish $1$. 
$\alpha$-values of $1, 0.5, 0.2, 0.1$ were tested. The $\lambda$ parameter for different values of $\alpha$ throughout training process is illustrated in \cref{fig:grl-curves}. 
Numerical results regarding the influence of $\lambda$ on the model performance are available in Section 5.1 Ablation Studies. 
          </p>
          <div class="item item-fullbody">
            <img src="figures/GRL-1.png" alt="overview_image" width="100%">
            <!-- add image --> 
            <!-- <h3 class="title is-3"> MS3D++</h3> -->
          </div>
          
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Few-shot learning</h2>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <p>
            
Motivated by other works, we try to use a few labels from target data to see if that
can further improve our results. In Table below, we present results across three scenarios.
Interestingly, we observed some improvements, particularly in adaptation towards robot
data, and slight enhancements in other datasets, although not as pronounced. This
could be attributed to robot data being smaller than nuScenes. Notably, we observed
particularly high improvements in the Pedestrian and Cyclist classes. Moreover, we
found that adding W → N led to higher improvements than CS64 → N. Remarkably, we
achieved these results using only 20 labeled target examples for this few-shot approach.

          </p>
          <div class="item item-fullbody">
            <img src="figures/few_shots.png" alt="overview_image" width="100%">
            <!-- add image --> 
            <!-- <h3 class="title is-3"> MS3D++</h3> -->
          </div>
          
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Mobile robot</h2>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <p>
            

            The mobile robot used to capture robot data is a wheeled last-mile delivery robot
            developed at TU Hamburg (see Figure 10). It is equipped with a Intel Core i7-7600U
            2x2.80 GHz CPU and a NVIDIA Volta GPU with 64 Tensor cores. Its sensors consist of
            two forward and backward facing Stereolabs ZED2 stereo cameras, four Intel RealSense
            D435 stereo cameras in the downward direction as well as a 16-channel Velodyne Puck
            (VLP-16) LiDAR. Since this work concerns LiDAR-based detection, only the LiDAR
            sensor is used for detection in this work.
            
            
          </p>
          <div class="item item-fullbody">
            <img src="figures/delivery_robot_prototype.jpg" alt="overview_image" width="100%">
            <!-- add image --> 
            <!-- <h3 class="title is-3"> MS3D++</h3> -->
          </div>
          <p>The training data was collected at the campuses and the neighborhoods During training
            and testing the data was randomly sampled from these two locations. Our dataset
            includes sequences from both outdoor (university sidewalks, small university roads, or parking lots) and indoor (the university building and a warehouse) scenarios. Most of
            the objects classes in these areas are pedestrians and cyclists. There are also numerous
            vehicles, mostly parked on the side of the road or driving across the university campus.
            Since the data was collected in scenarios similar to the intended use cases for such robots
            it can be seen as an accurate representation of the data that would be encountered in
            real-world operation. The data was labeled using the open-source annotation software
            SUSTechPoints [21]. In our train/test split, we used a similar number of scans as on
            the KITTI [9] data. We used 7000 scans for training and 3500 for testing.
            Together with this paper, we are planning to release the robot data we used in
            our scenarios. The complete dataset is a part of another project and will be published
            soon. In addition to the LiDAR data and class labels (used in this project), the full
            dataset will also include data from an RGBD camera, ground-facing stereo camera,
            an Inertial Measurement Unit (IMU), wheel odometry, and RTK GNSS. This diverse
            collection of sensors will offer a comprehensive perspective on the robot’s perception of
            its environment, providing valuable insights into the subtle aspects of robotic sensing
            capabilities.</p>
    </div>
  </div>
</section>





  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <h2 class="title is-3">Loss analysis</h2>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <p>
              The loss of our network consists of 2 components: detection loss and discriminator loss. While we optimize 
              the discriminators with L_C, we backpropagate this loss through the gradient reversal layer to the rest of the network.
               Thus, while the discriminator's objective is to minimize $\mathcal{L}_C$, the feature extractor and detection head benefit from 
               maximizing $\mathcal{L}_C$. In other words, the network aims to create features that are domain-invariant and useful for the object
                detection task. We can observe these losses in \cref{fig:losses_all}. Initially, we can see that the discriminator is capable of 
                distinguishing between the domains as the loss decreases and approaches 0. However, after approximately 30\% of the training, 
                the network begins to produce more invariant features. By about 40\% of the training, the network has learned to generate invariant 
                features, leading to a rapid increase in conditional discriminator loss. These features are still beneficial for detection, 
              resulting in a decrease in detection loss and a rapid overall loss reduction until the conditional discriminator loss plateaus again.
            </p>
          </div>
          <div class="container_subfig">
            <figure class="figure">
                <img src="figures/losses/det_loss-1.png" alt="Figure 1">
                <figcaption>Detection Loss</figcaption>
            </figure>
            <figure class="figure">
                <img src="figures/losses/disc_loss-1.png" alt="Figure 2">
                <figcaption>Discriminator Loss</figcaption>
            </figure>
            <figure class="figure">
                <img src="figures/losses/main_loss-1.png" alt="Figure 3">
                <figcaption>Main Loss</figcaption>
            </figure>
        </div>

  <!-- <div id="wrapper" style="text-align:center;  padding: 2em 0;"> 
    <div class="item item-fullbody">
      <img src="figures/losses/det_loss-1.png" alt="overview_image" width="55%">
      <h3 class="title is-3"> Detection Loss</h3>
    </div>
    <div class="item item-fullbody">
      <img src="figures/losses/disc_loss-1.png" alt="overview_image" width="55%">
      <h3 class="title is-3"> Discriminator loss</h3>
    </div>
    <div class="item item-fullbody">
      <img src="figures/losses/main_loss-1.png" alt="overview_image" width="55%">
      <h3 class="title is-3"> Overall loss</h3> -->
        </div>
      </div>
    </div>
  </section>









<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Qualitative Results</h2>
    </div>
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <p>
            
            Our method, UADA3D, demonstrated the ability to accurately train the model to recognize objects, irrespective of their proximity and domain. This enhanced detection capability is effective across all three classes: vehicles, pedestrians, and cyclists. Our approach significantly
             improves the model's ability to identify distant, close-by as well as heavily occluded objects. In the next figures,
              red bounding boxes indicate predictions, while green bounding boxes represent ground truth. We added blue squares to highlight 
              zoomed-in regions, and yellow dotted squares to highlight hard-to-detect far-away objects. As illustrated in 
              \cref{fig:zoomed_in1,fig:zoomed_in2,fig:zoomed_in3}, most of the models in comparison tend to miss far-away or 
              partially occluded objects, without producing many false positives. Additionally, they also tend to miss smaller non-vehicle objects.
               UADA3D on the other hand substantially enhances the model's robustness in these challenging scenarios, ensuring that such objects are
                not overlooked. Our method detects not only close and far away vehicles (\cref{fig:zoomed_in1,fig:zoomed_in2,fig:zoomed_in3}) 
                but also hard-to-see cyclists and pedestrians (zoomed-in regions in \cref{fig:zoomed_in1} and \cref{fig:zoomed_in2}, as well as 
                highlighted with the yellow box containing one instance in \cref{fig:zoomed_in1}).  Moreover, in \cref{fig:zoomed_in4}, 
                which presents results on robot data, we show UADA3D's comprehensive adaptation capabilities, where we successfully identify every 
                object in the scene, even though the domain gap is substantially large (self-driving car domain to mobile robot). This highlights 
                the effectiveness of our approach in diverse and demanding real-world applications.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

  <div class="columns is-centered has-text-centered">
    <h1 class="title is-3"> Qualitative results - small objects and pedestrians</h1>
  </div>

<div id="wrapper" style="text-align:center;"> 
    
    <!-- <video autoplay controls muted loop playsinline id="result_video_side" height="400px" width="auto" > 
        <source type="video/mp4" src="static/videos/2558_side-0-1.mp4" /> 
    </video> -->
    <div class="item item-fullbody">
      <img src="figures/detections_png/DTS1-1.png" alt="overview_image" width="95%">
      <!-- add image --> 
      <h3 class="title is-3"> DTS</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/LD1-1.png" alt="overview_image" width="95%">
      <!-- add image --> 
      <h3 class="title is-3"> L.D.</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/ST3D1-1.png" alt="overview_image" width="95%">
      <!-- add image --> 
      <h3 class="title is-3"> ST3D</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/MS3D1-1.png" alt="overview_image" width="95%">
      <!-- add image --> 
      <h3 class="title is-3"> MS3D++</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/UADA1-1.png" alt="overview_image" width="95%">
      <!-- add image --> 
      <h3 class="title is-3"> UADA3D</h3>
      
    </div>
  </div>
  </div>
</section>

    <!-- <video autoplay controls muted loop playsinline id="result_video_side"> 
        <source type="video/mp4" src="static/videos/000654_side-0-1.mp4" /> 
    </video>
    <video  autoplay controls muted loop playsinline id="result_video_side"> 
        <source type="video/mp4" src="static/videos/000242_side-0-1.mp4" /> 
    </video>

    <div class="clear"></div>
Tracking camera motion (horizontal)  -->

<!-- </div> -->
<!-- make vertical space -->
<br>


<section class="section">
  <div class="container is-max-desktop">

<div class="columns is-centered has-text-centered">

  <h1 class="title is-3"> Qualitative results - far away and ocluded objects</h1>
</div>
<br>

<div id="wrapper" style="text-align:center;"> 
  
  <!-- <video autoplay controls muted loop playsinline id="result_video_side" height="400px" width="auto" > 
      <source type="video/mp4" src="static/videos/2558_side-0-1.mp4" /> 
  </video> -->
  <div class="item item-fullbody">
    <img src="figures/detections_png/DTS2-1.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <h3 class="title is-3"> DTS</h3>
    
  </div>
  <div class="item item-fullbody">
    <img src="figures/detections_png/LD2-1.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <h3 class="title is-3"> L.D.</h3>
    
  </div>
  <div class="item item-fullbody">
    <img src="figures/detections_png/ST3D2-1.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <h3 class="title is-3"> ST3D</h3>
    
  </div>
  <div class="item item-fullbody">
    <img src="figures/detections_png/MS3D2-1.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <h3 class="title is-3"> MS3D++</h3>
    
  </div>
  <div class="item item-fullbody">
    <img src="figures/detections_png/UADA2-1.png" alt="overview_image" width="95%">
    <!-- add image --> 
    <h3 class="title is-3"> UADA3D</h3>
    
  </div>

</div>
</div>
</section>



<section class="section">
  <div class="container is-max-desktop">

  <div class="columns is-centered has-text-centered">
    <h1 class="title is-3"> Qualitative results - far away and ocluded objects part 2</h1>
  </div>

</div>

<br>

<div id="wrapper" style="text-align:center;"> 
    
    <!-- <video autoplay controls muted loop playsinline id="result_video_side" height="400px" width="auto" > 
        <source type="video/mp4" src="static/videos/2558_side-0-1.mp4" /> 
    </video> -->
    <div class="item item-fullbody">
      <img src="figures/detections_png/DTS3-1.png" alt="overview_image" width="55%">
      <!-- add image --> 
      <h3 class="title is-3"> DTS</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/LD3-1.png" alt="overview_image" width="55%">
      <!-- add image --> 
      <h3 class="title is-3"> L.D.</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/ST3D3-1.png" alt="overview_image" width="55%">
      <!-- add image --> 
      <h3 class="title is-3"> ST3D</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/MS3D3-1.png" alt="overview_image" width="55%">
      <!-- add image --> 
      <h3 class="title is-3"> MS3D++</h3>
      
    </div>
    <div class="item item-fullbody">
      <img src="figures/detections_png/UADA3-1.png" alt="overview_image" width="55%">
      <!-- add image --> 
      <h3 class="title is-3"> UADA3D</h3>
    </div>
  </div>
</section>













<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{wozniak2024uada3d,
        title={UADA3D: Unsupervised Adversarial Domain Adaptation for 3D Object Detection with Sparse LiDAR and Large Domain Gaps},
        author={Wozniak, Maciej K and Hansson, Mattias and Thiel, Marko and Jensfelt, Patric},
        journal={arXiv preprint arXiv:2403.17633},
        year={2024}
      }</code></pre>
  </div>
</section>



<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
</footer>


</body>
</html>
